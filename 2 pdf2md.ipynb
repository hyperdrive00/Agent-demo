{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5b2e72",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# PDF转Markdown工具\n",
    "\n",
    "本notebook使用MinerU库将PDF文件转换为Markdown格式。\n",
    "详情请参考\n",
    "https://github.com/opendatalab/MinerU\n",
    "\n",
    "## 步骤1：设置环境变量\n",
    "首先设置模型下载源，如果遇到模型下载问题可以使用modelscope作为备选源。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64fe29dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 如果在下载模型时遇到问题，可以设置此环境变量\n",
    "os.environ['MINERU_MODEL_SOURCE'] = \"modelscope\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d366a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 步骤2：创建目录结构\n",
    "\n",
    "创建必要的目录结构：\n",
    "- `pdf_articles`：存放待处理的PDF文件\n",
    "- `md_articles`：存放转换后的Markdown文件\n",
    "\n",
    "同时导入必要的库文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b59e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# 创建存放示例PDF文件的目录\n",
    "pdf_files_dir = Path(\"pdf_articles\")\n",
    "pdf_files_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 创建输出目录\n",
    "output_dir = Path(\"md_articles\")\n",
    "output_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90dec9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 步骤3：扫描待处理文件\n",
    "\n",
    "扫描`pdf_articles`目录中的所有PDF和图像文件，生成待处理文件列表。\n",
    "\n",
    "支持的文件格式：\n",
    "- PDF文件：`.pdf`\n",
    "- 图像文件：`.png`, `.jpeg`, `.jpg`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d4c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "待解析文档数量: 5\n",
      "待解析文档列表: [WindowsPath('pdf_articles/article1.pdf'), WindowsPath('pdf_articles/article2.pdf'), WindowsPath('pdf_articles/article3.pdf'), WindowsPath('pdf_articles/article4.pdf'), WindowsPath('pdf_articles/article5.pdf')]\n"
     ]
    }
   ],
   "source": [
    "# 定义支持的文件类型\n",
    "pdf_suffixes = [\".pdf\"]\n",
    "image_suffixes = [\".png\", \".jpeg\", \".jpg\"]\n",
    "\n",
    "# 在源目录中查找所有支持的文档\n",
    "doc_path_list = []\n",
    "for doc_path in pdf_files_dir.glob('*'):\n",
    "    if doc_path.suffix in pdf_suffixes + image_suffixes:\n",
    "        doc_path_list.append(doc_path)\n",
    "\n",
    "print(f\"待解析文档数量: {len(doc_path_list)}\")\n",
    "print(f\"待解析文档列表: {doc_path_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f89103",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 步骤4：定义PDF解析函数\n",
    "\n",
    "这是核心的PDF解析模块，包含两个主要函数：\n",
    "\n",
    "### `do_parse` 函数\n",
    "- 负责实际的PDF解析过程\n",
    "- 支持两种后端：pipeline（管道模式）和vlm（视觉语言模型）\n",
    "- 可以输出多种格式：Markdown、JSON、图像等\n",
    "\n",
    "### `parse_doc` 函数\n",
    "- 高级接口，简化了PDF解析的调用\n",
    "- 支持批量处理多个文档\n",
    "- 提供丰富的参数配置选项\n",
    "\n",
    "参考：https://github.com/opendatalab/MinerU/blob/master/demo/demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76bd8aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HYPER\\anaconda3\\envs\\litagent\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2025-07-13 15:34:03.388\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mmineru.backend.vlm.predictor\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m35\u001b[0m - \u001b[33m\u001b[1msglang is not installed. If you are not using sglang, you can ignore this warning.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "from mineru.cli.common import convert_pdf_bytes_to_bytes_by_pypdfium2, prepare_env, read_fn\n",
    "from mineru.data.data_reader_writer import FileBasedDataWriter\n",
    "from mineru.utils.draw_bbox import draw_layout_bbox, draw_span_bbox\n",
    "from mineru.utils.enum_class import MakeMode\n",
    "from mineru.backend.vlm.vlm_analyze import doc_analyze as vlm_doc_analyze\n",
    "from mineru.backend.pipeline.pipeline_analyze import doc_analyze as pipeline_doc_analyze\n",
    "from mineru.backend.pipeline.pipeline_middle_json_mkcontent import union_make as pipeline_union_make\n",
    "from mineru.backend.pipeline.model_json_to_middle_json import result_to_middle_json as pipeline_result_to_middle_json\n",
    "from mineru.backend.vlm.vlm_middle_json_mkcontent import union_make as vlm_union_make\n",
    "from mineru.utils.models_download_utils import auto_download_and_get_model_root_path\n",
    "\n",
    "def do_parse(\n",
    "    output_dir,  # 存储解析结果的输出目录\n",
    "    pdf_file_names: list[str],  # 待解析的PDF文件名列表\n",
    "    pdf_bytes_list: list[bytes],  # 待解析的PDF字节数据列表\n",
    "    p_lang_list: list[str],  # 每个PDF的语言列表，默认为'ch'（中文）\n",
    "    backend=\"pipeline\",  # 解析PDF的后端，默认为'pipeline'\n",
    "    parse_method=\"auto\",  # 解析PDF的方法，默认为'auto'\n",
    "    formula_enable=True,  # 启用公式解析\n",
    "    table_enable=True,  # 启用表格解析\n",
    "    server_url=None,  # vlm-sglang-client后端的服务器URL\n",
    "    f_draw_layout_bbox=True,  # 是否绘制布局边界框\n",
    "    f_draw_span_bbox=True,  # 是否绘制文本边界框\n",
    "    f_dump_md=True,  # 是否输出markdown文件\n",
    "    f_dump_middle_json=True,  # 是否输出中间JSON文件\n",
    "    f_dump_model_output=True,  # 是否输出模型输出文件\n",
    "    f_dump_orig_pdf=True,  # 是否输出原始PDF文件\n",
    "    f_dump_content_list=True,  # 是否输出内容列表文件\n",
    "    f_make_md_mode=MakeMode.MM_MD,  # 制作markdown内容的模式，默认为MM_MD\n",
    "    start_page_id=0,  # 解析的起始页面ID，默认为0\n",
    "    end_page_id=None,  # 解析的结束页面ID，默认为None（解析到文档末尾）\n",
    "):\n",
    "\n",
    "    if backend == \"pipeline\":\n",
    "        for idx, pdf_bytes in enumerate(pdf_bytes_list):\n",
    "            new_pdf_bytes = convert_pdf_bytes_to_bytes_by_pypdfium2(pdf_bytes, start_page_id, end_page_id)\n",
    "            pdf_bytes_list[idx] = new_pdf_bytes\n",
    "\n",
    "        infer_results, all_image_lists, all_pdf_docs, lang_list, ocr_enabled_list = pipeline_doc_analyze(pdf_bytes_list, p_lang_list, parse_method=parse_method, formula_enable=formula_enable,table_enable=table_enable)\n",
    "\n",
    "        for idx, model_list in enumerate(infer_results):\n",
    "            model_json = copy.deepcopy(model_list)\n",
    "            pdf_file_name = pdf_file_names[idx]\n",
    "            local_image_dir, local_md_dir = prepare_env(output_dir, pdf_file_name, parse_method)\n",
    "            image_writer, md_writer = FileBasedDataWriter(local_image_dir), FileBasedDataWriter(local_md_dir)\n",
    "\n",
    "            images_list = all_image_lists[idx]\n",
    "            pdf_doc = all_pdf_docs[idx]\n",
    "            _lang = lang_list[idx]\n",
    "            _ocr_enable = ocr_enabled_list[idx]\n",
    "            middle_json = pipeline_result_to_middle_json(model_list, images_list, pdf_doc, image_writer, _lang, _ocr_enable, formula_enable)\n",
    "\n",
    "            pdf_info = middle_json[\"pdf_info\"]\n",
    "\n",
    "            pdf_bytes = pdf_bytes_list[idx]\n",
    "            if f_draw_layout_bbox:\n",
    "                draw_layout_bbox(pdf_info, pdf_bytes, local_md_dir, f\"{pdf_file_name}_layout.pdf\")\n",
    "\n",
    "            if f_draw_span_bbox:\n",
    "                draw_span_bbox(pdf_info, pdf_bytes, local_md_dir, f\"{pdf_file_name}_span.pdf\")\n",
    "\n",
    "            if f_dump_orig_pdf:\n",
    "                md_writer.write(\n",
    "                    f\"{pdf_file_name}_origin.pdf\",\n",
    "                    pdf_bytes,\n",
    "                )\n",
    "\n",
    "            if f_dump_md:\n",
    "                image_dir = str(os.path.basename(local_image_dir))\n",
    "                md_content_str = pipeline_union_make(pdf_info, f_make_md_mode, image_dir)\n",
    "                md_writer.write_string(\n",
    "                    f\"{pdf_file_name}.md\",\n",
    "                    md_content_str,\n",
    "                )\n",
    "\n",
    "            if f_dump_content_list:\n",
    "                image_dir = str(os.path.basename(local_image_dir))\n",
    "                content_list = pipeline_union_make(pdf_info, MakeMode.CONTENT_LIST, image_dir)\n",
    "                md_writer.write_string(\n",
    "                    f\"{pdf_file_name}_content_list.json\",\n",
    "                    json.dumps(content_list, ensure_ascii=False, indent=4),\n",
    "                )\n",
    "\n",
    "            if f_dump_middle_json:\n",
    "                md_writer.write_string(\n",
    "                    f\"{pdf_file_name}_middle.json\",\n",
    "                    json.dumps(middle_json, ensure_ascii=False, indent=4),\n",
    "                )\n",
    "\n",
    "            if f_dump_model_output:\n",
    "                md_writer.write_string(\n",
    "                    f\"{pdf_file_name}_model.json\",\n",
    "                    json.dumps(model_json, ensure_ascii=False, indent=4),\n",
    "                )\n",
    "\n",
    "            logger.info(f\"local output dir is {local_md_dir}\")\n",
    "    else:\n",
    "        if backend.startswith(\"vlm-\"):\n",
    "            backend = backend[4:]\n",
    "\n",
    "        f_draw_span_bbox = False\n",
    "        parse_method = \"vlm\"\n",
    "        for idx, pdf_bytes in enumerate(pdf_bytes_list):\n",
    "            pdf_file_name = pdf_file_names[idx]\n",
    "            pdf_bytes = convert_pdf_bytes_to_bytes_by_pypdfium2(pdf_bytes, start_page_id, end_page_id)\n",
    "            local_image_dir, local_md_dir = prepare_env(output_dir, pdf_file_name, parse_method)\n",
    "            image_writer, md_writer = FileBasedDataWriter(local_image_dir), FileBasedDataWriter(local_md_dir)\n",
    "            middle_json, infer_result = vlm_doc_analyze(pdf_bytes, image_writer=image_writer, backend=backend, server_url=server_url)\n",
    "\n",
    "            pdf_info = middle_json[\"pdf_info\"]\n",
    "\n",
    "            if f_draw_layout_bbox:\n",
    "                draw_layout_bbox(pdf_info, pdf_bytes, local_md_dir, f\"{pdf_file_name}_layout.pdf\")\n",
    "\n",
    "            if f_draw_span_bbox:\n",
    "                draw_span_bbox(pdf_info, pdf_bytes, local_md_dir, f\"{pdf_file_name}_span.pdf\")\n",
    "\n",
    "            if f_dump_orig_pdf:\n",
    "                md_writer.write(\n",
    "                    f\"{pdf_file_name}_origin.pdf\",\n",
    "                    pdf_bytes,\n",
    "                )\n",
    "\n",
    "            if f_dump_md:\n",
    "                image_dir = str(os.path.basename(local_image_dir))\n",
    "                md_content_str = vlm_union_make(pdf_info, f_make_md_mode, image_dir)\n",
    "                md_writer.write_string(\n",
    "                    f\"{pdf_file_name}.md\",\n",
    "                    md_content_str,\n",
    "                )\n",
    "\n",
    "            if f_dump_content_list:\n",
    "                image_dir = str(os.path.basename(local_image_dir))\n",
    "                content_list = vlm_union_make(pdf_info, MakeMode.CONTENT_LIST, image_dir)\n",
    "                md_writer.write_string(\n",
    "                    f\"{pdf_file_name}_content_list.json\",\n",
    "                    json.dumps(content_list, ensure_ascii=False, indent=4),\n",
    "                )\n",
    "\n",
    "            if f_dump_middle_json:\n",
    "                md_writer.write_string(\n",
    "                    f\"{pdf_file_name}_middle.json\",\n",
    "                    json.dumps(middle_json, ensure_ascii=False, indent=4),\n",
    "                )\n",
    "\n",
    "            if f_dump_model_output:\n",
    "                model_output = (\"\\n\" + \"-\" * 50 + \"\\n\").join(infer_result)\n",
    "                md_writer.write_string(\n",
    "                    f\"{pdf_file_name}_model_output.txt\",\n",
    "                    model_output,\n",
    "                )\n",
    "\n",
    "            logger.info(f\"local output dir is {local_md_dir}\")\n",
    "\n",
    "\n",
    "def parse_doc(\n",
    "        path_list: list[Path],\n",
    "        output_dir,\n",
    "        lang=\"ch\",\n",
    "        backend=\"pipeline\",\n",
    "        method=\"auto\",\n",
    "        server_url=None,\n",
    "        start_page_id=0,\n",
    "        end_page_id=None\n",
    "):\n",
    "    \"\"\"\n",
    "        参数说明:\n",
    "        path_list: 待解析的文档路径列表，可以是PDF或图像文件。\n",
    "        output_dir: 存储解析结果的输出目录。\n",
    "        lang: 语言选项，默认为'ch'，可选值包括['ch', 'ch_server', 'ch_lite', 'en', 'korean', 'japan', 'chinese_cht', 'ta', 'te', 'ka']。\n",
    "            输入PDF中的语言（如果已知）以提高OCR准确性。可选。\n",
    "            仅适用于后端设置为\"pipeline\"的情况\n",
    "        backend: 解析PDF的后端:\n",
    "            pipeline: 更通用。\n",
    "            vlm-transformers: 更通用。\n",
    "            vlm-sglang-engine: 更快（引擎）。\n",
    "            vlm-sglang-client: 更快（客户端）。\n",
    "            未指定方法时，默认使用pipeline。\n",
    "        method: 解析PDF的方法:\n",
    "            auto: 基于文件类型自动确定方法。\n",
    "            txt: 使用文本提取方法。\n",
    "            ocr: 对基于图像的PDF使用OCR方法。\n",
    "            未指定方法时，默认使用'auto'。\n",
    "            仅适用于后端设置为\"pipeline\"的情况。\n",
    "        server_url: 当后端为`sglang-client`时，需要指定server_url，例如:`http://127.0.0.1:30000`\n",
    "        start_page_id: 解析的起始页面ID，默认为0\n",
    "        end_page_id: 解析的结束页面ID，默认为None（解析到文档末尾）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_name_list = []\n",
    "        pdf_bytes_list = []\n",
    "        lang_list = []\n",
    "        for path in path_list:\n",
    "            file_name = str(Path(path).stem)\n",
    "            pdf_bytes = read_fn(path)\n",
    "            file_name_list.append(file_name)\n",
    "            pdf_bytes_list.append(pdf_bytes)\n",
    "            lang_list.append(lang)\n",
    "        do_parse(\n",
    "            output_dir=output_dir,\n",
    "            pdf_file_names=file_name_list,\n",
    "            pdf_bytes_list=pdf_bytes_list,\n",
    "            p_lang_list=lang_list,\n",
    "            backend=backend,\n",
    "            parse_method=method,\n",
    "            server_url=server_url,\n",
    "            start_page_id=start_page_id,\n",
    "            end_page_id=end_page_id\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee307e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 步骤5：执行PDF解析\n",
    "\n",
    "现在开始执行PDF解析过程。使用`parse_doc`函数处理之前扫描到的所有PDF文件。\n",
    "\n",
    "**参数配置：**\n",
    "- `doc_path_list`: 待处理的文档路径列表\n",
    "- `output_dir`: 输出目录（md_articles）\n",
    "- `backend=\"pipeline\"`: 使用pipeline后端进行解析\n",
    "\n",
    "**解析过程：**\n",
    "1. 读取PDF文件内容\n",
    "2. 进行文档分析和内容提取\n",
    "3. 生成Markdown格式的输出文件\n",
    "4. 保存相关的中间文件和图像\n",
    "\n",
    "**输出结果：**\n",
    "- Markdown文件（.md）\n",
    "- 原始PDF文件副本\n",
    "- 布局和文本边界框可视化PDF\n",
    "- 中间JSON文件\n",
    "- 提取的图像文件\n",
    "\n",
    "注：第一次运行需要下载模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52fa720e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-13 15:34:33.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmineru.backend.pipeline.pipeline_analyze\u001b[0m:\u001b[36mdoc_analyze\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mBatch 1/1: 55 pages/55 pages\u001b[0m\n",
      "\u001b[32m2025-07-13 15:34:33.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmineru.backend.pipeline.pipeline_analyze\u001b[0m:\u001b[36mbatch_image_analyze\u001b[0m:\u001b[36m187\u001b[0m - \u001b[1mgpu_memory: 16 GB, batch_ratio: 16\u001b[0m\n",
      "\u001b[32m2025-07-13 15:34:33.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmineru.backend.pipeline.model_init\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1mDocAnalysis init, this may take some times......\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\HYPER\\.cache\\modelscope\\hub\\models\\OpenDataLab\\PDF-Extract-Kit-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:36,172 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\HYPER\\.cache\\modelscope\\hub\\models\\OpenDataLab\\PDF-Extract-Kit-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:39,403 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\HYPER\\.cache\\modelscope\\hub\\models\\OpenDataLab\\PDF-Extract-Kit-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:43,467 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\HYPER\\.cache\\modelscope\\hub\\models\\OpenDataLab\\PDF-Extract-Kit-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,527 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\HYPER\\.cache\\modelscope\\hub\\models\\OpenDataLab\\PDF-Extract-Kit-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:49,329 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\HYPER\\.cache\\modelscope\\hub\\models\\OpenDataLab\\PDF-Extract-Kit-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:52,191 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "\u001b[32m2025-07-13 15:34:52.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmineru.backend.pipeline.model_init\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m182\u001b[0m - \u001b[1mDocAnalysis init done!\u001b[0m\n",
      "\u001b[32m2025-07-13 15:34:52.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmineru.backend.pipeline.pipeline_analyze\u001b[0m:\u001b[36mcustom_model_init\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mmodel init cost: 18.744927883148193\u001b[0m\n",
      "Layout Predict: 100%|██████████| 55/55 [00:02<00:00, 21.62it/s]\n",
      "MFD Predict: 100%|██████████| 55/55 [00:03<00:00, 14.39it/s]\n",
      "MFR Predict: 100%|██████████| 1763/1763 [00:10<00:00, 169.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\HYPER\\.cache\\modelscope\\hub\\models\\OpenDataLab\\PDF-Extract-Kit-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:35:12,333 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\HYPER\\.cache\\modelscope\\hub\\models\\OpenDataLab\\PDF-Extract-Kit-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:35:15,207 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "OCR-det ch: 100%|██████████| 182/182 [00:10<00:00, 16.90it/s]\n",
      "Table Predict:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\HYPER\\.cache\\modelscope\\hub\\models\\OpenDataLab\\PDF-Extract-Kit-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:35:28,870 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Table Predict: 100%|██████████| 6/6 [00:04<00:00,  1.46it/s]\n",
      "OCR-rec Predict: 100%|██████████| 8770/8770 [00:29<00:00, 298.94it/s]\n",
      "Processing pages:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\HYPER\\.cache\\modelscope\\hub\\models\\OpenDataLab\\PDF-Extract-Kit-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:36:03,416 - modelscope - INFO - Target directory already exists, skipping creation.\n",
      "Processing pages: 100%|██████████| 12/12 [00:04<00:00,  2.89it/s]\n",
      "\u001b[32m2025-07-13 15:36:05.088\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_parse\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mlocal output dir is md_articles\\article1\\auto\u001b[0m\n",
      "Processing pages: 100%|██████████| 9/9 [00:00<00:00, 19.09it/s]\n",
      "\u001b[32m2025-07-13 15:36:05.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_parse\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mlocal output dir is md_articles\\article2\\auto\u001b[0m\n",
      "Processing pages: 100%|██████████| 11/11 [00:00<00:00, 21.42it/s]\n",
      "\u001b[32m2025-07-13 15:36:06.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_parse\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mlocal output dir is md_articles\\article3\\auto\u001b[0m\n",
      "Processing pages: 100%|██████████| 14/14 [00:01<00:00, 13.42it/s]\n",
      "\u001b[32m2025-07-13 15:36:08.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_parse\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mlocal output dir is md_articles\\article4\\auto\u001b[0m\n",
      "Processing pages: 100%|██████████| 9/9 [00:00<00:00, 16.18it/s]\n",
      "\u001b[32m2025-07-13 15:36:09.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdo_parse\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mlocal output dir is md_articles\\article5\\auto\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "parse_doc(doc_path_list, output_dir, backend=\"pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
